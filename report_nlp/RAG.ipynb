{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758cbd27",
   "metadata": {},
   "source": [
    "## Two step RAG method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4488d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/litigation/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c14ed5",
   "metadata": {},
   "source": [
    "### Loading and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae3cafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = 'groundtruth_classifications.xlsx'\n",
    "text_data = 'full_data_filtered.csv'\n",
    "\n",
    "data = pd.read_csv(text_data)\n",
    "examples = pd.read_excel(few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823c9820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "      <th>Company</th>\n",
       "      <th>Climate</th>\n",
       "      <th>Litigation</th>\n",
       "      <th>Climate Litigation</th>\n",
       "      <th>General risk</th>\n",
       "      <th>Specific lawsuit(s)</th>\n",
       "      <th>Note</th>\n",
       "      <th>Paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIG_0000005272-19-000023</td>\n",
       "      <td>AIG</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pricing for our products is subject to our abi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AIG_0000005272-19-000023</td>\n",
       "      <td>AIG</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We are exposed to certain risks if we are unab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AIG_0000005272-19-000023</td>\n",
       "      <td>AIG</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If our businesses do not perform well and/or t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIG_0000005272-19-000023</td>\n",
       "      <td>AIG</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We recognize that climate change has implicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevron_0000093410-24-000014</td>\n",
       "      <td>Chevron</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Petroleum industry operations and profitabilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-climate litigation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Climate litigation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File name  Company Climate              Litigation  \\\n",
       "0       AIG_0000005272-19-000023      AIG      No                     Yes   \n",
       "1       AIG_0000005272-19-000023      AIG      No                     Yes   \n",
       "2       AIG_0000005272-19-000023      AIG      No                      No   \n",
       "3       AIG_0000005272-19-000023      AIG     Yes                      No   \n",
       "4   Chevron_0000093410-24-000014  Chevron     Yes                      No   \n",
       "..                           ...      ...     ...                     ...   \n",
       "61                           NaN      NaN     NaN                     NaN   \n",
       "62                           NaN      NaN     NaN                     NaN   \n",
       "63                           NaN      NaN     NaN                     NaN   \n",
       "64                           NaN      NaN     NaN  Non-climate litigation   \n",
       "65                           NaN      NaN     NaN      Climate litigation   \n",
       "\n",
       "   Climate Litigation General risk Specific lawsuit(s) Note  \\\n",
       "0                  No          Yes                  No  NaN   \n",
       "1                  No          Yes                  No  NaN   \n",
       "2                  No           No                  No  NaN   \n",
       "3                  No          Yes                  No  NaN   \n",
       "4                  No          Yes                  No  NaN   \n",
       "..                ...          ...                 ...  ...   \n",
       "61                NaN          NaN                 NaN  NaN   \n",
       "62                NaN          NaN                 NaN  NaN   \n",
       "63                NaN          NaN                 NaN  NaN   \n",
       "64                NaN           32                 NaN  NaN   \n",
       "65                NaN           29                 NaN  NaN   \n",
       "\n",
       "                                            Paragraph  \n",
       "0   Pricing for our products is subject to our abi...  \n",
       "1   We are exposed to certain risks if we are unab...  \n",
       "2   If our businesses do not perform well and/or t...  \n",
       "3   We recognize that climate change has implicati...  \n",
       "4   Petroleum industry operations and profitabilit...  \n",
       "..                                                ...  \n",
       "61                                                NaN  \n",
       "62                                                NaN  \n",
       "63                                                NaN  \n",
       "64                                                NaN  \n",
       "65                                                NaN  \n",
       "\n",
       "[66 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf9eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/4mkn2pts7yg8xb1_dfd6g01c0000gn/T/ipykernel_99093/2978246190.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  examples = examples.replace({'Yes': 1, 'No': 0})\n"
     ]
    }
   ],
   "source": [
    "# Clean up the examples\n",
    "examples = examples[examples['File name'].notna()]\n",
    "examples = examples.drop(columns=['Note'])\n",
    "examples['year'] = examples['File name'].str.extract(r'-(\\d{2})-')\n",
    "examples = examples.replace({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71d0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "litigation_examples = examples.drop(columns=['Climate', 'Litigation', 'General risk', 'Specific lawsuit(s)', 'File name'])\n",
    "litigation_examples.rename(columns={'Paragraph': 'text', 'Company': 'company', 'Climate Litigation': 'climate_litigation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b439388",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'text': 'text', 'folder': 'company'}, inplace=True)\n",
    "data.drop(columns=['folderfiletext'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4bbd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 528 documents and 61 ground truth examples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(data)} documents and {len(litigation_examples)} ground truth examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea57293",
   "metadata": {},
   "source": [
    "### Chunking the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f0a3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab980ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (44218 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking ground truth examples...\n",
      "Created 145734 document chunks and 61 ground truth chunks\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunking documents...\")\n",
    "expanded_rows = []\n",
    "for _, row in data.iterrows():\n",
    "    expanded_rows.extend(utils.tokenize_and_chunk(row, tokenizer))\n",
    "df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "print(\"Chunking ground truth examples...\")\n",
    "groundtruth_expanded = []\n",
    "for _, row in litigation_examples.iterrows():\n",
    "    groundtruth_expanded.extend(utils.tokenize_and_chunk(row, tokenizer))\n",
    "groundtruth_df = pd.DataFrame(groundtruth_expanded)\n",
    "\n",
    "print(f\"Created {len(df)} document chunks and {len(groundtruth_df)} ground truth chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fe489",
   "metadata": {},
   "source": [
    "### Setting up embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a0bc322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b021c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   1%|          | 125/14574 [01:37<3:07:11,  1.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEncoding document embeddings...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m doc_embeddings = utils.encode_in_batches(df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].tolist(), embedding_model, batch_size=\u001b[32m10\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEncoding ground truth embeddings...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m gt_embeddings = utils.encode_in_batches(groundtruth_df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].tolist(), embedding_model, batch_size=\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/litigation_project/report_nlp/utils.py:45\u001b[39m, in \u001b[36mencode_in_batches\u001b[39m\u001b[34m(texts, model, batch_size)\u001b[39m\n\u001b[32m     43\u001b[39m global_index = i + j\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     embedding = model.encode(\n\u001b[32m     46\u001b[39m         text,\n\u001b[32m     47\u001b[39m         show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     48\u001b[39m         convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     49\u001b[39m         normalize_embeddings=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     50\u001b[39m     )\n\u001b[32m     51\u001b[39m     local_embeddings.append(embedding)\n\u001b[32m     52\u001b[39m     valid_indices.append(global_index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/litigation/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:685\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    682\u001b[39m features.update(extra_features)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     out_features = \u001b[38;5;28mself\u001b[39m.forward(features, **kwargs)\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    687\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/litigation/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:758\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    757\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28minput\u001b[39m = module(\u001b[38;5;28minput\u001b[39m, **module_kwargs)\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/litigation/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/litigation/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/litigation/lib/python3.13/site-packages/sentence_transformers/models/Pooling.py:172\u001b[39m, in \u001b[36mPooling.forward\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooling_mode_mean_tokens \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooling_mode_mean_sqrt_len_tokens:\n\u001b[32m    169\u001b[39m     input_mask_expanded = (\n\u001b[32m    170\u001b[39m         attention_mask.unsqueeze(-\u001b[32m1\u001b[39m).expand(token_embeddings.size()).to(token_embeddings.dtype)\n\u001b[32m    171\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, \u001b[32m1\u001b[39m)\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# If tokens are weighted (by WordWeights layer), feature 'token_weights_sum' will be present\u001b[39;00m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_weights_sum\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Encoding document embeddings...\")\n",
    "doc_embeddings = utils.encode_in_batches(df['text'].tolist(), embedding_model, batch_size=10)\n",
    "\n",
    "print(\"Encoding ground truth embeddings...\")\n",
    "gt_embeddings = utils.encode_in_batches(groundtruth_df['text'].tolist(), embedding_model, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings, valid_doc_idx = doc_embeddings\n",
    "gt_embeddings, valid_gt_idx = gt_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"doc_embeddings.npy\", doc_embeddings)\n",
    "np.save(\"gt_embeddings.npy\", gt_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6cac3c",
   "metadata": {},
   "source": [
    "### Using the AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f54ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = np.load(\"doc_embeddings.npy\")\n",
    "gt_embeddings = np.load(\"gt_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d60ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embedding\"] = list(doc_embeddings)\n",
    "groundtruth_df[\"embedding\"] = list(gt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49edaeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_groups = df.groupby('company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e248b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_ROUTER_KEY = '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'\n",
    "client = openai.OpenAI(\n",
    "    api_key=OPEN_ROUTER_KEY,\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5502ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing company: AEP with 470 chunks\n",
      "Index(['company', 'year', 'text', 'embedding'], dtype='object')\n",
      "Classifying 100 candidate chunks for AEP\n",
      "[ERROR] Failed to classify paragraph: ENVIRONMENTAL ISSUES\n",
      "We are implementing a substantial capital investment progra...\n",
      "Exception: Error code: 401 - {'error': {'message': 'No auth credentials found', 'code': 401}}\n",
      "[AEP][Year 2014] Chunk 1/100: ENVIRONMENTAL ISSUES\n",
      "We are implementing a substan... -> SKIPPED\n",
      "[ERROR] Failed to classify paragraph: 376\n",
      "The Registrant Subsidiaries are engaged in litigation about environmental is...\n",
      "Exception: Error code: 401 - {'error': {'message': 'No auth credentials found', 'code': 401}}\n",
      "[AEP][Year 2014] Chunk 2/100: 376\n",
      "The Registrant Subsidiaries are engaged in lit... -> SKIPPED\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m os.makedirs(nested_output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Run classification for this year subset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results_df = utils.run_rag_classification_for_company(\n\u001b[32m     10\u001b[39m     embedding_model=embedding_model,\n\u001b[32m     11\u001b[39m     groundtruth_df=groundtruth_df,\n\u001b[32m     12\u001b[39m     client=client,\n\u001b[32m     13\u001b[39m     company_df=year_df.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     14\u001b[39m     company_name=company,\n\u001b[32m     15\u001b[39m     retrieval_k=\u001b[32m100\u001b[39m,\n\u001b[32m     16\u001b[39m     example_k=\u001b[32m5\u001b[39m,\n\u001b[32m     17\u001b[39m     start_index=\u001b[32m0\u001b[39m,\n\u001b[32m     18\u001b[39m     output_dir=nested_output_dir\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/coding/litigation_project/report_nlp/utils.py:274\u001b[39m, in \u001b[36mrun_rag_classification_for_company\u001b[39m\u001b[34m(embedding_model, groundtruth_df, client, company_df, company_name, retrieval_k, example_k, start_index, output_dir)\u001b[39m\n\u001b[32m    270\u001b[39m         print(f\"[{company_name}][Year {result['year']}] Progress saved after {i+1} chunks\")\n\u001b[32m    272\u001b[39m     time.sleep(3)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m # Final save\n\u001b[32m    275\u001b[39m results_df = pd.DataFrame(results)\n\u001b[32m    276\u001b[39m final_path = os.path.join(\n\u001b[32m    277\u001b[39m     output_dir,\n\u001b[32m    278\u001b[39m     f\"rag_results_{company_name}_{results_df['year'].iloc[0] if not results_df.empty else 'NA'}.csv\"\n\u001b[32m    279\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for company, company_df in company_groups:\n",
    "    # Ensure company-level folder exists\n",
    "    for year, year_df in company_df.groupby('year'):\n",
    "        # Define nested output directory for company and year\n",
    "        nested_output_dir = os.path.join(\"rag_results\", company, str(year))\n",
    "        os.makedirs(nested_output_dir, exist_ok=True)\n",
    "\n",
    "        # Run classification for this year subset\n",
    "        results_df = utils.run_rag_classification_for_company(\n",
    "            embedding_model=embedding_model,\n",
    "            groundtruth_df=groundtruth_df,\n",
    "            client=client,\n",
    "            company_df=year_df.reset_index(drop=True),\n",
    "            company_name=company,\n",
    "            retrieval_k=100,\n",
    "            example_k=5,\n",
    "            start_index=0,\n",
    "            output_dir=nested_output_dir\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litigation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
